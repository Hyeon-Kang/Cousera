Neural Networks and Deep Learning

https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome



190913 Week01

Introduction to deep learning

개요 소개

1. 고양이 인식기 예제
2. 딥러 동작, 파라미터 튜닝 등 최적화
3. 딥러닝 모델 전략, 프로젝트 구조 (엔드 투 엔드 딥러닝)
  언제 써야 할까??, 응용 사례??

4. CNN (주로 이미지 분야에 적용)
5. sequence 모델을 자연어 처리 (NLP)나 다른 분야에 적용하는 (RNN, LSTM 등)



What is a neural network?

Deep Learning,, refers to training Neural Networks

신경망이란?

ReLu 함수 (Rectified Linear Unit) 정류된?? -> 0을 최대값으로 갖는

신경세포(예측 요소)를 쌓으면서 더 큰 신경망을 갖춘다.

이렇게 갖춰진 신경망에 입력과 출력을 트레이닝세트에 도입 시키면 된다.
(hidden layer는 알아서 해결해준다!)

x 값을 토대로 y값을 예측

지도학습의 예시?




Supervised Learning with Neural Networks

자주 쓰이는 예시

* 지도학습 (Supervised Learning)

1) 집값 예측
2) 온라인 광고 예측 (유저 정보 : 입력, 적절한 광고 : 출력)
3) Image (사진 분석, tagging)
4) 음성분석
5) 문장 번역
6) 자율 주행
등등등!

[supervised_learning_example.jpg]

가장 중요한 것은 신경망이 올바른 x,y 값을 갖는 것



standard nn : 전통적인 방식

Convolutional nn : 이미지 데이터

Recurrent nn : 시간적인 요소를 포함한 일차원적 데이터 ???

[Neual Network example.jpg]



structed data VS unstructed data1

정형화된 데이터는 우리가 알고 있는 DB 테이블 처럼 명확한 데이터가

비정형 데이터 audio, image, text 등 형식이 없는 데이터 (휴리스틱)



신경망이 지도학습에 미친 영향




Why is Deep Learning taking off?

왜 딥러닝이 떴을까?

초기 딥러닝 분야는 방대한 데이터와 처리능력으로 성능을 올렸다.

알고리즘의 진화 sigmoid func -> ReLu func

시그모이드 함수는 양 끝의 기울기가 0에 가까워진다.

문제는 기울기가 0에 근접한 부분에서 학습 속도가 매우 느려진다는 것

gradient descent


RelU 함수를 도입할 경우 양수영역의 값이 0으로 줄어드는 확률이 급격히 감소
(학습 속도 유지)

-> 알고리즘 발전




nn 성능 증가? 데이터 양 늘리기? 네트워크 증가?

이런 행위는 한계가 있다. (데이터의 한계 or 네트워크가 너무 커져서 연산이 버거움)



About this Course

Week01

기본기 다지는 과정
1) 딥러닝 소개
2) 문제풀이

Week02
1) 신경망 프로그래밍 구조 : foward propagation, back propagation
2) 신경망을 효과적으로 도입하는 방법?
3) 프로그래밍, 알고리즘 도입

Week03
1) 신경망 네트워크 프로그래밍의 프레임워 -> hidden layer 코드

Week04
1) 심층신경망, 심화학습



week 01 종료







week 02 시작

https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Z8j0R/binary-classification


Binary Classification

 - cat recognition


 하나의 학습표본 => (x, y)

 x = x의 차원을 가진 특징벡터 (채널이 분리된 이미지의 모든 픽셀값을 행렬화)

 y = 0 or 1 을 갖는 레이블

 m = 학습 표본의 수 (m = m_train), 테스트 표본의 수 (m_test)




Logistic Regression


이진분류 문제

x가 고양이 사진 (특징벡터)인 경우 y 는 x가 고양이일 확률

output y =  (sigmoid) (w(transpose)x + b)

sigmoid(z) = 1/1+e^(-z)

e^-z 에서 z가 클 수록 0에 근접함

따라서 1/1에 근접하므로 z가 클 수록 시그모이드 곡선이 1에 근접함

반대로 z가 음수의 큰 값을 가지게 된 다면 1/1+매우 큰 값 = 0에 근접

W와 B 파라미터


W : nx dimensional

B : real number

Loss(error) function 제곱오류랑 비슷함

L function이 유효값 y를 갖는 경우 djfaksk 정확히 y(hat) 결과값을 산출하는지 정의할 때 사용

L(y(hat),y) = 1/2(y(hat),y)^2

Lfunction을 사용해서 안정적인 최적화 가능 (최적화 하기 더 쉬워짐)

if y=1 : L(y(hat), y) = -log y(hat)
y(hat) 을 최대한 크게 (1과 가깝게) 하는 것이 좋다.

if y=0 : L(y(hat), y) = -log(1-y(hat))
만약 y가 0인 경우, loss 함수의 첫 항은 0이 됨

L(y(hay), y) = -(y log y(hat) + (1-y) log(1-y(hat)))
                요기가 0


러닝 단계에서 loss 함수를 작게 만들려고 하는 것은 (if y=0)

log (1-y(hat)) 의 값이 최대한 크게 하는 것과 같다.

앞에 '-' 부호를 고려

=> loss 함수가 y(hat)의 값을 최대한 작게 하려 함과 같다.


y 값을 0으로 지정하면 loss 함수는 y(hat)이 최대한 0에 가까운 값이 나오도록 함



Cost function : J(w, b)

[cost_function.jpg]

[cost_function_fix.png]

로지스틱 회귀분석법은 자체적으로 아주 작은 신경망 네트워크로 볼 수 있다.

로지스틱 회귀분석 추가 : https://3months.tistory.com/327

                       : https://medium.com/qandastudy/mathpresso-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%84%B0%EB%94%94-4-%ED%9A%8C%EA%B7%80-%EB%B6%84%EC%84%9D-regression-2-4f938f1f1c2d


정리 질문

Q:
What is the difference between the cost function and the loss function for logistic regression?

A:
The loss function computes the error for a single training example;
the cost function is the average of the loss functions of the entire training set.


loss func : 한 텀의 학습 예제 오류
cost func : 전체 학습 에제 오류의 평균, 주요파라미터를 통한 cost func 계산 (w, b)







Gradient Descent

로지스틱 회귀분석법
  - loss function : single trainig set에서 오류 계산
  - cost function : 모든 training set에서 오류 계산
